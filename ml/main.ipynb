{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run spacy download ru_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\me\\Dev\\Python\\X5Case\\ml\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"ru_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.prefer_gpu() # Используется ли ускорение на gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\"\"\"Используйте трансформерные модели, если нужна максимальная нагрузка на GPU\n",
    "Стандартные маленькие модели (_sm) часто не используют GPU по полной, для максимума берите _trf версии \n",
    "(например, ru_core_news_trf), которые построены на PyTorch и используют CUDA для ускорения.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbs: ['использовать', 'использовать', 'брать', 'построить', 'использовать']\n"
     ]
    }
   ],
   "source": [
    "# Analyze syntax\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train = r\"data\\train.csv\"\n",
    "df = pd.read_csv(path_to_train, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from spacy.tokens import DocBin\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_save(data_array, output_path):\n",
    "    # Создаем объект DocBin для сериализации документов spaCy\n",
    "    db = DocBin()\n",
    "    \n",
    "    # Проходим по всем строкам данных (каждая строка - текст и строка с аннотациями)\n",
    "    for (text, annotations_str) in data_array:\n",
    "        # Преобразуем строковое представление аннотаций в Python-объект (список кортежей)\n",
    "        annotations = ast.literal_eval(annotations_str)\n",
    "\n",
    "        end_text = len(text)  # Длина текста для корректировки границ сущностей\n",
    "\n",
    "        # Создаем объект Doc из исходного текста (токенизация без анализа)\n",
    "        doc = nlp.make_doc(text)\n",
    "\n",
    "        ents = []\n",
    "        for start, end, label in annotations:\n",
    "            # Корректируем границы, чтобы не выходить за пределы текста\n",
    "            end = end_text if end > end_text else end\n",
    "            start = 0 if start < 0 else start\n",
    "            \n",
    "            # Создаем span - сегмент с меткой, с режимом расширения, чтобы избежать пропусков из-за ошибок разметки\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"expand\")\n",
    "            ents.append(span)\n",
    "\n",
    "        # Присваиваем полученные сущности документу\n",
    "        doc.ents = ents\n",
    "        \n",
    "        # Добавляем документ в объект DocBin (формат для эффективного хранения и обмена)\n",
    "        db.add(doc)\n",
    "\n",
    "    # Сохраняем сериализованный набор документов в файл на диск\n",
    "    db.to_disk(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Путь для сохранения обучающего датасета в формате spacy\n",
    "output_train_path = \"data/train.spacy\"\n",
    "# Путь для сохранения валидационного (dev) датасета\n",
    "output_dev_path = \"data/dev.spacy\"\n",
    "\n",
    "# Деление данных DataFrame на тренировочную и дев выборки в соотношении 0.7 к 0.3\n",
    "data = df.to_numpy()\n",
    "\n",
    "# Перемешиваем\n",
    "np.random.seed(42)\n",
    "shuffled_indices = np.random.permutation(len(data))\n",
    "data = data[shuffled_indices]\n",
    "\n",
    "split_idx = math.floor(len(data) * 0.7)  # Индекс, где делим массив\n",
    "\n",
    "# Разделяем данные\n",
    "train_data = data[:split_idx]  # 70% для тренировки\n",
    "dev_data = data[split_idx:]    # 30% для валидации\n",
    "\n",
    "# Конвертируем и сохраняем тренировочный датасет\n",
    "convert_and_save(train_data, output_train_path)\n",
    "# Конвертируем и сохраняем валидационный датасет\n",
    "convert_and_save(dev_data, output_dev_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример считывания из spacy файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем данные из файла\n",
    "with open(r\"data\\train.spacy\", \"rb\") as f:\n",
    "    doc_bin_bytes = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DocBin().from_bytes(doc_bin_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(db.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abon\n",
      "abon O\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    if doc.text == \"abon\":\n",
    "        print(doc.text)\n",
    "        # Можно получить сущности, токены и др.\n",
    "        for ent in doc.ents:\n",
    "            print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.tokens import DocBin\n",
    "\n",
    "# # Чтение и загрузка тренировочного файла\n",
    "# with open(r\"data/train.spacy\", \"rb\") as f:\n",
    "#     train_doc_bin_bytes = f.read()\n",
    "\n",
    "# train_db = DocBin().from_bytes(train_doc_bin_bytes)\n",
    "\n",
    "# # Чтение и загрузка валидирующего файла\n",
    "# with open(r\"data/dev.spacy\", \"rb\") as f:\n",
    "#     dev_doc_bin_bytes = f.read()\n",
    "\n",
    "# dev_db = DocBin().from_bytes(dev_doc_bin_bytes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тест обученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"models_gpu/model-last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(r\"абрикосы 500г global village\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"фруктовое пбре без сахара\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "абрикосы B-TYPE\n",
      "500 B-BRAND\n",
      "г I-BRAND\n",
      "global B-BRAND\n",
      "village I-BRAND\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
